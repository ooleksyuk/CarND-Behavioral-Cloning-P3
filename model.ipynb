{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Keras\n",
      "Importing NumPy\n",
      "Importing CSV and CV\n",
      "Importing library to shuffle and split data\n",
      "Importing library to process data\n",
      "All set importing libraries...\n"
     ]
    }
   ],
   "source": [
    "#Import libraries needed for the project\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout2D, ELU, Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D, Conv2D\n",
    "from keras.layers.core import Lambda\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "print(\"Importing Keras\")\n",
    "import numpy as np\n",
    "print(\"Importing NumPy\")\n",
    "import csv\n",
    "import cv2\n",
    "print(\"Importing CSV and CV\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "print(\"Importing library to shuffle and split data\")\n",
    "from PIL import Image\n",
    "print(\"Importing library to process data\")\n",
    "print(\"All set importing libraries...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Project Variables\n",
    "samples = []\n",
    "X = []\n",
    "y = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# For model\n",
    "nb_epoch = 30 # because of the version of TensorFlow library I am using variable name for epochs `nb_epochs`, it has been deprecated now\n",
    "validation_split = 0.2 # Keep 20% of data for validatoin purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get CSV file path out of string\n",
    "def csv_to_samples(csv_filepath):\n",
    "    print(\"Open CSV file to load images data...\")\n",
    "    print(\"CSV file path\", csv_filepath)\n",
    "    samples = []\n",
    "    with open(csv_filepath) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            samples.append(line)\n",
    "    # Pop the first item on the list to get rid of list headers\n",
    "    removed_line = samples.pop(0)\n",
    "    print(\"removing unused line\", removed_line)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cols():\n",
    "    # Number of columns in the resized image\n",
    "    return 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rows():\n",
    "    # Number of rows in the resized image\n",
    "    return 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def steering_correction():\n",
    "    # How much to retate image on y axes if it comes from left or right camera\n",
    "    return 0.275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_size():\n",
    "    return 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cropped_pixels():\n",
    "    # how many pixels from the top to crop off of the image\n",
    "    # these pixels should contain irrelevant to driving information\n",
    "    # sky, trees, hirizon,...\n",
    "    return 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def random_brightness(image):\n",
    "    \"\"\"\n",
    "    Returns an image with a random degree of brightness.\n",
    "    :param image: Image represented as a numpy array.\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    brightness = .25 + np.random.uniform()\n",
    "    image[:,:,2] = image[:,:,2] * brightness\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Process image before converting into array\n",
    "def pre_process_image(image, path_id, cols, rows):\n",
    "    source_path = image[path_id]\n",
    "    filename = source_path.split('/')[-1]\n",
    "    current_path = 'data/IMG/' + filename\n",
    "    img = Image.open(current_path)\n",
    "    img = np.array(img)\n",
    "    \n",
    "    # Crop only road part of the image as we do not need to train network on sky and \n",
    "    # other surraundings not related to the road and increase brightness\n",
    "    img = random_brightness(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)[cropped_pixels():,:,1]  \n",
    "    # Resize all images to be same size to form matrixes of same shape\n",
    "    return cv2.resize(img, (cols, rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def steering(arr):\n",
    "    # data about steering is located in the array at position 3\n",
    "    return arr[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate array of data from CSV lines of samples\n",
    "def generate_array(samples, batch_size):\n",
    "    X, y = [], []\n",
    "    print(\"Preprocessing images and Loading data...\")\n",
    "    for sample in samples:\n",
    "        # central camera, no steering correction needed\n",
    "        # path to the image from it is located in the array: sample at position 0\n",
    "        X.append(pre_process_image(sample, 0, cols(), rows()))\n",
    "        y.append(float(steering(sample)))\n",
    "\n",
    "        # left camera, need to do steering correction from the right\n",
    "        # path to the image from it is located in the array: sample at position 1\n",
    "        X.append(pre_process_image(sample, 1, cols(), rows()))\n",
    "        y.append(float(steering(sample)) + steering_correction())\n",
    "\n",
    "        # right camera, need to do steering correction from the left\n",
    "        # path to the image from it is located in the array: sample at position 2\n",
    "        X.append(pre_process_image(sample, 2, cols(), rows()))\n",
    "        y.append(float(steering(sample)) - steering_correction())\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(\"Data Size: \", X.shape, y.shape)\n",
    "    print(\"Loading data complete\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Post process image array data\n",
    "def post_process(X, y):\n",
    "    # flip the image data horizontally and append the new data to the training data\n",
    "    X = np.concatenate([X, X[:,:,::-1]])\n",
    "    # negate the steering wheel angles of the flipped images\n",
    "    y = np.concatenate([y, -y])\n",
    "\n",
    "    # shuffle the training data\n",
    "    X, y = sk_shuffle(X, y)\n",
    "\n",
    "    X_train = X[:,:,:,None]\n",
    "    y_train = y[:]\n",
    "\n",
    "    print(\"Training Data Size: \", X_train.shape, y_train.shape)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open CSV file to load images data...\n",
      "CSV file path data/driving_log.csv\n",
      "removing unused line ['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
      "Preprocessing images and Loading data...\n",
      "Data Size:  (24108, 11, 32) (24108,)\n",
      "Loading data complete\n",
      "Training Data Size:  (48216, 11, 32, 1) (48216,)\n"
     ]
    }
   ],
   "source": [
    "# Get data for training ready\n",
    "samples = csv_to_samples('data/driving_log.csv');\n",
    "X, y = generate_array(samples, batch_size());\n",
    "X_train, y_train = post_process(X=X, y=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "Normalization (Lambda)           (None, 11, 32, 1)     0           lambda_input_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 11, 32, 2)     4           Normalization[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 3, 8, 2)       0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 3, 8, 2)       0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 48)            0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             49          flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 53\n",
      "Trainable params: 53\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Neural Network Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/127.5 - 1., input_shape=(rows(), cols(), 1), name='Normalization'))\n",
    "model.add(Conv2D(2, 1, 1, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D((4, 4), (4, 4), 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "# .summary function can provide data about the model input paramt and architecture, for reporting purposes\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Use Adam otimizer when compiling the model and mean square function to calculate loss\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "# Train the model on the training data with 25 epochs\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=batch_size(), \n",
    "    verbose=1, \n",
    "    validation_split=validation_split, \n",
    "    nb_epoch=nb_epoch,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for the new drive.py file\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
